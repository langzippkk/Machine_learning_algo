{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# The following code uses the following concepts/functions from NumPy, and it\n",
    "# may help to brush up on them from the documentation.\n",
    "#\n",
    "# - Broadcasting\n",
    "# - np.pad\n",
    "# - np.reshape\n",
    "# - np.tensordot (optional)\n",
    "# - np.newaxis\n",
    "# - np.moveaxis\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "class relu:\n",
    "    \n",
    "    def function(self, X):\n",
    "        '''Return elementwise relu values'''\n",
    "        return(np.maximum(np.zeros(X.shape), X))\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        # An example of broadcasting\n",
    "        return((X >= 0).astype(int))\n",
    "        \n",
    "class no_act:\n",
    "    \"\"\"Implement a no activation function and derivative\"\"\"\n",
    "    \n",
    "    def function(self, X):\n",
    "        return(X)\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        return(np.ones(X.shape))\n",
    "\n",
    "# Set of allowed/implemented activation functions\n",
    "ACTIVATIONS = {'relu': relu,\n",
    "               'no_act': no_act}    \n",
    "    \n",
    "class CNNLayer:\n",
    "    \"\"\"\n",
    "    Implement a class that processes a single CNN layer.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    Additional two comments by Amitabh for Spring, 2020: \n",
    "    \n",
    "    (1) I emphasized slightly different \n",
    "    notation in my class presentations; this notebook uses the older version.  \n",
    "    For one, in class I have been using d(a_i) to denote (del L)/(del a_i).\n",
    "    Also in class I have emphasized that the nodes/neurons correspond to filters,\n",
    "    and the image/pixel values correspond to outputs when these filters are applied\n",
    "    at particular receptive field.  This notebook, however, calls each application\n",
    "    a \"neuron\".  So if an image corresponding to the jth layer consists of \n",
    "    500 x 500 x 3 pixels, the notebook imagines they are the outputs of 500 x 500 x 3 \n",
    "    \"neurons\". Which ignores the fact that there are actually only 3 distinct neurons, \n",
    "    (corresponding two 3 filters), each of which has been applied 500 x 500 times on\n",
    "    an input image.\n",
    "    \n",
    "    (2) This starter code uses the older idea that the activation function is part\n",
    "    of the layer.  I intend to change this in future.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    Let i be the index on the neurons in the ith layer, and j be the index on the \n",
    "    neurons in the next outer layer.  (Following Russell-Norvig notation.) Implement \n",
    "    the following:\n",
    "    \n",
    "    0. __init__: Initalize filters.\n",
    "    \n",
    "    1. forward step: Input a_i values.  Output a_j values.  Make copies of a_i values \n",
    "       and in_j values since needed in backward_step and filter_gradient.\n",
    "    \n",
    "    2. backward_step: Input (del L)/(del a_j) values.  Output (del L)/(del a_i).\n",
    "    \n",
    "    3. filter_gradient: Input (del L)/(del a_j) values. Output (del L)/(del w_{ij}) values.\n",
    "    \n",
    "    4. update: Given learning rate, update filter weights. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, filter_shape, activation='no_act', stride = 1):\n",
    "        \"\"\"\n",
    "        Initialize filters.\n",
    "        \n",
    "        filter_shape is (height of filter, width of filter, depth of filter). Depth \n",
    "        of filter should match depth of the forward_step input X.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_filters = n\n",
    "        self.stride = stride\n",
    "        self.filter_shape = filter_shape\n",
    "        try:\n",
    "            self.filter_height = filter_shape[0]\n",
    "            self.filter_width = filter_shape[1]\n",
    "            self.filter_depth = filter_shape[2]\n",
    "        except:\n",
    "            raise Exception(f'Unexpected filter shape {filter_shape}')\n",
    "        try:\n",
    "            # Create an object of the activation class\n",
    "            self.activation = ACTIVATIONS[activation]() \n",
    "        except:\n",
    "            raise Exception(f'Unknown activation: {activation}')\n",
    "        self.filters = self.filters_init()\n",
    "        self.biases = self.biases_init()\n",
    "        self.num_examples = None \n",
    "        # Set num_of_examples during forward step, and use to verify\n",
    "        # consistency during backward step.  Similarly the data height, \n",
    "        # width, and depth.\n",
    "        self.data_height = None\n",
    "        self.data_width = None\n",
    "        self.data_depth = None\n",
    "        self.data_with_pads = None\n",
    "        self.in_j = None  # the in_j values for next layer.\n",
    "        self.im2col = None\n",
    "        \n",
    "    def filters_init(self):\n",
    "        return np.random.random((self.num_filters, self.filter_height,\n",
    "                                 self.filter_width, self.filter_depth))\n",
    "    \n",
    "    def biases_init(self):\n",
    "        return np.random.random(self.num_filters)\n",
    "    \n",
    "    def set_filters(self, filters, biases):\n",
    "        \"\"\"Set filters to given weights.\n",
    "        \n",
    "           Useful in debugging.\"\"\"\n",
    "        if filters.shape != (self.num_filters, self.filter_height,\n",
    "                                 self.filter_width, self.filter_depth):\n",
    "            raise Exception(f'Mismatched filter shapes: stored '\n",
    "                            f'{self.num_filters} {self.filter_shape} vs '\n",
    "                            f'{filters.shape}.')\n",
    "        if biases.shape != (self.num_filters,):\n",
    "            raise Exception((f'Mismatched biases: stored '\n",
    "                             f'{self.num_filters} vs '\n",
    "                             f'{biases.shape}.'))\n",
    "        self.filters = filters.copy()\n",
    "        self.biases = biases.copy()\n",
    "        \n",
    "    def forward_step(self, X, pad_height=0, pad_width=0):\n",
    "        \"\"\"\n",
    "        Implement a forward step.\n",
    "        \n",
    "        X.shape is (number of examples, height of input, width of input, depth of input).\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Store shape values to verify consistency during backward step\n",
    "            self.num_examples = X.shape[0]\n",
    "            self.data_height = X.shape[1]\n",
    "            self.data_width = X.shape[2]\n",
    "            self.data_depth = X.shape[3]\n",
    "        except:\n",
    "            raise Exception(f'Unexpected data shape {X.shape}')\n",
    "        if self.data_depth != self.filter_depth:\n",
    "            raise Exception(f'Depth mismatch: filter depth {self.filter_depth}'\n",
    "                            f' data depth {self.data_depth}')\n",
    "        self.pad_height = pad_height\n",
    "        self.pad_width = pad_width\n",
    "        self.input_height = self.data_height + 2 * self.pad_height\n",
    "        self.input_width = self.data_width + 2 * self.pad_width\n",
    "        \n",
    "        # Add pad to X.  Only add pads to the 1, 2 (ht, width) axes of X, \n",
    "        # not to the 0, 4 (num examples, depth) axes.\n",
    "        # 'constant' implies 0 is added as pad.\n",
    "        X = np.pad(X, ((0,0),(pad_height, pad_height), \n",
    "                      (pad_width, pad_width), (0,0)), 'constant')\n",
    "        \n",
    "        # Save a copy for computing filter_gradient\n",
    "        self.a_i = X.copy()  #\n",
    "        \n",
    "        # Get height, width after padding\n",
    "        height = X.shape[1]\n",
    "        width = X.shape[2]\n",
    "\n",
    "        # Don't include pad in formula because height includes it.\n",
    "        output_height = ((height - self.filter_height)/self.stride + 1)\n",
    "        output_width = ((width - self.filter_width)/self.stride + 1)    \n",
    "        if (\n",
    "            output_height != int(output_height) or \n",
    "            output_width != int(output_width)\n",
    "        ):\n",
    "            raise Exception(f\"Filter doesn't fit: {output_height} x {output_width}\")\n",
    "        else:\n",
    "            output_height = int(output_height)\n",
    "            output_width = int(output_width)\n",
    "            \n",
    "        #####################################################################\n",
    "        # There are two ways to convolve the filters with X.\n",
    "        # 1. Using the im2col method described in Stanford 231 notes.\n",
    "        # 2. Using NumPy's tensordot method.\n",
    "        #\n",
    "        # (1) requires more code.  (2) requires understanding how tensordot\n",
    "        # works.  Most likely tensordot is more efficient.  To illustrate both,\n",
    "        # in the code below data_tensor is constructed using (1) and \n",
    "        # new_data_tensor is constructed using (2).  You may use either.\n",
    "            \n",
    "        # Stanford's im2col method    \n",
    "        # Construct filter tensor and add biases\n",
    "        filter_tensor = self.filters.reshape(self.num_filters, -1)\n",
    "        filter_tensor = np.hstack((self.biases.reshape((-1,1)), filter_tensor))\n",
    "        # Construct the data tensor\n",
    "        # The im2col_length does not include the bias terms\n",
    "        # Biases are later added to both data and filter tensors\n",
    "        im2col_length = self.filter_height * self.filter_width * self.filter_depth\n",
    "        num_outputs = output_height * output_width\n",
    "        data_tensor = np.empty((self.num_examples, num_outputs, im2col_length))\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                hs = h * self.stride\n",
    "                ws = w * self.stride\n",
    "                data_tensor[:,h*output_width + w, :] = X[:,hs:hs+self.filter_height,\n",
    "                                    ws:ws+self.filter_width,:].reshape(\n",
    "                                        (self.num_examples,-1))  \n",
    "        # add bias-coeffs to data tensor\n",
    "        self.im2col = data_tensor.copy()\n",
    "        data_tensor = np.concatenate((np.ones((self.num_examples, num_outputs, 1)),\n",
    "                                 data_tensor), axis=2)\n",
    "        \n",
    "        ## the im2col result, will use in backward\n",
    "        ## data_tensor: (num_examples,out_w*out_h,(C*kernel_h*kernel_w+1))\n",
    "        ## filter_tensor: (num_filters, C*kernel_h*kernel_w+1)\n",
    "        output_tensor = np.tensordot(data_tensor, filter_tensor, axes=([2],[1]))\n",
    "        output_tensor = output_tensor.reshape(\n",
    "            (self.num_examples,output_height,output_width,self.num_filters))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # NumPy's tensordot based method\n",
    "        new_output_tensor = np.empty((self.num_examples, output_height, \n",
    "                                      output_width, self.num_filters))\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                hs = h * self.stride\n",
    "                ws = w * self.stride\n",
    "                new_output_tensor[:,h,w,:] = np.tensordot(\n",
    "                                                X[:, # example\n",
    "                                                  hs:hs+self.filter_height, # height\n",
    "                                                  ws:ws+self.filter_width,  # width\n",
    "                                                  : # depth\n",
    "                                                ], \n",
    "                                                self.filters[:, # filter \n",
    "                                                             :, # height\n",
    "                                                             :, # width\n",
    "                                                             :  # depth\n",
    "                                                ], \n",
    "                                                axes = ((1,2,3),(1,2,3))\n",
    "                                              )\n",
    "                # Add bias term\n",
    "                new_output_tensor[:,h,w,:] = (new_output_tensor[:,h,w,:] + \n",
    "                                              self.biases)\n",
    "        # Check both methods give the same answer\n",
    "        assert np.array_equal(output_tensor, new_output_tensor)\n",
    "                \n",
    "        \n",
    "        self.in_j = output_tensor.copy() # Used in backward_step.\n",
    "        output_tensor = self.activation.function(output_tensor) # a_j values\n",
    "        return(output_tensor)\n",
    "      \n",
    "    def backward_step(self, D):\n",
    "        \"\"\"\n",
    "        Implement the backward step and return (del L)/(del a_i). \n",
    "        \n",
    "        Given D=(del L)/(del a_j) values return (del L)/(del a_i) values.  \n",
    "        D (delta) is of shape (number of examples, height of output (i.e., the \n",
    "        a_j values), width of output, depth of output).\n",
    "        \n",
    "        Note that in our tests we will assume that (del L)/(del a_i) has\n",
    "        shape corresponding to the padded X used in forward_step, and not the\n",
    "        unpadded X.  Strictly speaking, only the unpadded X gradient is needed\n",
    "        for gradient descent, but here we ask you to compute the gradient for the \n",
    "        padded X.  All our tests below, and in our grading will make this assumption.\n",
    "        \n",
    "        YOU MAY ASSUME STRIDE IS SET TO 1.        \n",
    "        \"\"\"\n",
    "         ##  A = self.in_j : (self.num_examples,output_height,output_width,self.num_filters)\n",
    "        try:\n",
    "            num_examples = D.shape[0]\n",
    "            delta_height = D.shape[1]\n",
    "            delta_width = D.shape[2]\n",
    "            delta_depth = D.shape[3]\n",
    "        except:\n",
    "            raise Exception(f'Unexpected delta shape {D.shape}')\n",
    "        if num_examples != self.num_examples:\n",
    "            raise Exception(f'Number of examples changed from forward step: '\n",
    "                             f'{self.num_examples} vs {num_examples}')\n",
    "        if delta_depth != self.num_filters:\n",
    "            raise Exception(f'Depth mismatch: number of filters {self.num_filters}' \n",
    "                            f' delta depth {delta_depth}')\n",
    "        # Make a copy so that we can change it\n",
    "        prev_delta = D.copy()\n",
    "        if prev_delta.ndim != 4:\n",
    "            raise Exception(f'Unexpected number of dimensions {D.ndim}')\n",
    "        new_delta = None\n",
    "        \n",
    "        ####################################################################\n",
    "        # WRITE YOUR CODE HERE\n",
    "        #D (delta) is of shape (number of examples, height of output (i.e., the a_j values), width of output, depth of output = #of filters)\n",
    "        ## new_delta :(number of examples, height of input, width of input, depth of input).\n",
    "        ## self.filters : (num_filters, filter_height, filter_width, filter_depth=input_depth)\n",
    "        new_delta_temp = np.zeros((self.num_examples,self.filter_depth*self.filter_height*self.filter_width,D.shape[1],D.shape[2]))\n",
    "        for i in range(D.shape[0]):\n",
    "            for c in range(D.shape[3]):\n",
    "                for h in range(D.shape[1]):\n",
    "                    for w in range(D.shape[2]):\n",
    "                        new_delta_temp[i,:,w,h] += (self.filters[c,:,:,:]*D[i,h,w,c]).flatten()              \n",
    "        ## new_delta_temp : (num_examples,C*filter_h*filter_w,out_height,out_width) - > \n",
    "        ## new_delta(number of examples, height of input, width of input, depth of input).\n",
    "        new_delta =  np.zeros((self.a_i.shape))\n",
    "        N,H,W,C = self.a_i.shape\n",
    "        for i in range(D.shape[0]):\n",
    "            for w in range(D.shape[2]):\n",
    "                for h in range(D.shape[1]):\n",
    "                    slice1 = new_delta_temp[i,:,h,w].reshape((self.filter_height,self.filter_width,self.filter_depth))\n",
    "                    ## a slice of filter_h,filter_w,filer_depth                   \n",
    "                    top = h\n",
    "                    down = min(H,top+self.filter_height)\n",
    "                    left = w\n",
    "                    right = min(W,left+self.filter_width)      \n",
    "                    top1 = max(0,-h)\n",
    "                    down1 = min(self.filter_height,H-down+self.filter_height)\n",
    "                    left1 = max(0,-w)\n",
    "                    right1 = min(self.filter_width,W-right+self.filter_width)\n",
    "                    new_delta[i,top:down,left:right,:] += slice1[top1:down1,left1:right1,:]           \n",
    "        \n",
    "        return(new_delta)\n",
    "    \n",
    "    def filter_gradient(self, D):\n",
    "        \"\"\"\n",
    "        Return the filter_gradient.\n",
    "        \n",
    "        D = (del L)/(del a_j) has shape (num_examples, height, width, depth=num_filters)\n",
    "        The filter_gradient (del L)/(del w_{ij}) has shape (num_filters, filter_height, \n",
    "        filter_width, filter_depth=input_depth)\n",
    "        \n",
    "        YOU MAY ASSUME STRIDE IS SET TO 1.\n",
    "        \n",
    "        \"\"\"\n",
    "         \n",
    "        if DEBUG and D.ndim != 4:\n",
    "            raise Exception(f'D has {D.ndim} dimensions instead of 4.')\n",
    "        # D depth should match number of filters\n",
    "        D_depth = D.shape[3]\n",
    "        if DEBUG:\n",
    "            if D_depth != self.num_filters:\n",
    "                raise Exception(f'D depth {D_depth} != num_filters'\n",
    "                                f' {self.num_filters}')\n",
    "            if D.shape[0] != self.num_examples:\n",
    "                raise Exception(f'D num_examples {D.shape[0]} !='\n",
    "                                f'num_examples {self.num_examples}')\n",
    "        f_gradient = None\n",
    "        ####################################################################\n",
    "        # WRITE YOUR CODE HERE\n",
    "        ## self.ai:(number of examples, height of input, width of input, depth of input).\n",
    "        ## D: (num_examples, height, width,  D_depth)\n",
    "        ## self.im2col: (num_examples,out_w*out_h,(C*kernel_h*kernel_w))\n",
    "        ## output f_gradient: (num_filters, filter_height, filter_width, filter_depth=input_depth)\n",
    "        f_gradient = np.zeros((D_depth,self.filter_height,self.filter_width,self.filter_depth))\n",
    "        bias_temp = np.zeros((D_depth))\n",
    "        for i in range(D.shape[0]):\n",
    "            for c in range(D_depth):\n",
    "                for h in range(D.shape[1]):\n",
    "                    for w in range(D.shape[2]):\n",
    "                        temp = self.im2col[i,(h*D.shape[0] + w),:].reshape(self.filter_depth,self.filter_height,self.filter_width)\n",
    "                        f_gradient[c,:,:,:] += temp*D[i,h,w,c]\n",
    "                        bias_temp[c] += D[i,h,w,c]\n",
    "        ## f_gradient = np.concatenate((f_gradient,bias_temp), axis=0)\n",
    "        return f_gradient\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1,2,3,4])\n",
    "A.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def test_CNNLayer_cs231():\n",
    "    \"\"\"\n",
    "    Longer test example based on demo at Stanford's CS231\n",
    "    that has depth > 1, stride = 2, but only tests forward steps.\n",
    "    \"\"\"\n",
    "\n",
    "    W0 = np.empty((3,3,3), float)\n",
    "    W1 = np.empty((3,3,3), float)\n",
    "    X = np.empty((7,7,3), float)\n",
    "    X_no_pad = np.empty((5,5,3), float)\n",
    "\n",
    "    # filter W0\n",
    "    W0[:, :, 0] = np.array([\n",
    "            [-1, -1, 0],\n",
    "            [-1, 0, 1],\n",
    "            [-1, 1, 1]])\n",
    "    W0[:, :, 1] = np.array([\n",
    "            [-1, -1, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 1, 1]])\n",
    "    W0[:, :, 2] = np.array([\n",
    "            [1, -1, 1],\n",
    "            [0, -1, 1],\n",
    "            [0, -1, 0]])\n",
    "    b0 = 1\n",
    "        \n",
    "    # filter W1    \n",
    "    W1[:, :, 0] = np.array([\n",
    "            [0, 0, 0],\n",
    "            [-1, 1, 1],\n",
    "            [1, 1, 1]])\n",
    "    W1[:, :, 1] = np.array([\n",
    "            [0, -1, 1],\n",
    "            [-1, 0, 1],\n",
    "            [0, -1, 1]])\n",
    "    W1[:, :, 2] = np.array([\n",
    "            [0, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [-1, 0, -1]])\n",
    "    b1 = 0    \n",
    "        \n",
    "    X[:,:,0] = np.array([\n",
    "            [0] * 7,\n",
    "            [0, 0, 0 ,0, 2, 1, 0],\n",
    "            [0, 0, 0 ,0, 2, 2, 0],\n",
    "            [0, 1, 2 ,1, 2, 1, 0],\n",
    "            [0, 2, 0 ,0, 1, 0, 0],\n",
    "            [0, 1, 1 ,0, 0, 1, 0],\n",
    "            [0] * 7])\n",
    "    X[:,:,1] = np.array([\n",
    "            [0] * 7,\n",
    "            [0, 0, 0 ,1, 1, 0, 0],\n",
    "            [0, 1, 2 ,0, 1, 0, 0],\n",
    "            [0, 2, 0 ,2, 2, 0, 0],\n",
    "            [0, 0, 1 ,1, 0, 0, 0],\n",
    "            [0, 2, 2 ,2, 2, 1, 0],\n",
    "            [0] * 7])\n",
    "    X[:,:,2] = np.array([\n",
    "            [0] * 7,\n",
    "            [0, 1, 2 ,1, 0, 0, 0],\n",
    "            [0, 0, 1 ,2, 1, 2, 0],\n",
    "            [0, 2, 2 ,1, 0, 2, 0],\n",
    "            [0, 0, 2 ,1, 2, 0, 0],\n",
    "            [0, 0, 1 ,1, 1, 0, 0],\n",
    "            [0] * 7])\n",
    "    \n",
    "    X_no_pad[:,:,0] = np.array([\n",
    "            [0, 0, 0, 2, 1],\n",
    "            [0, 0, 0, 2, 2],\n",
    "            [1, 2, 1, 2, 1],\n",
    "            [2, 0, 0, 1, 0],\n",
    "            [1, 1, 0, 0, 1]])\n",
    "    X_no_pad[:,:,1] = np.array([\n",
    "            [0, 0 ,1, 1, 0],\n",
    "            [1, 2 ,0, 1, 0],\n",
    "            [2, 0 ,2, 2, 0],\n",
    "            [0, 1 ,1, 0, 0],\n",
    "            [2, 2 ,2, 2, 1]])\n",
    "    X_no_pad[:,:,2] = np.array([\n",
    "            [1, 2 ,1, 0, 0],\n",
    "            [0, 1 ,2, 1, 2],\n",
    "            [2, 2 ,1, 0, 2],\n",
    "            [0, 2 ,1, 2, 0],\n",
    "            [0, 1 ,1, 1, 0]]) \n",
    "    \n",
    "    output_relu = np.array(\n",
    "[[[[ 5.,  0.],\n",
    "   [ 3.,  6.],\n",
    "   [ 0.,  1.]],\n",
    "  [[ 6.,  5.],\n",
    "   [ 0.,  2.],\n",
    "   [ 0.,  0.]],\n",
    "  [[ 3.,  5.],\n",
    "   [ 1.,  0.],\n",
    "   [ 2.,  0.]]],\n",
    " [[[ 5.,  0.],\n",
    "   [ 3.,  6.],\n",
    "   [ 0.,  1.]],\n",
    "  [[ 6.,  5.],\n",
    "   [ 0.,  2.],\n",
    "   [ 0.,  0.]],\n",
    "  [[ 3.,  5.],\n",
    "   [ 1.,  0.],\n",
    "   [ 2.,  0.]]],\n",
    " [[[ 5.,  0.],\n",
    "   [ 3.,  6.],\n",
    "   [ 0.,  1.]],\n",
    "  [[ 6.,  5.],\n",
    "   [ 0.,  2.],\n",
    "   [ 0.,  0.]],\n",
    "  [[ 3.,  5.],\n",
    "   [ 1.,  0.],\n",
    "   [ 2.,  0.]]]]\n",
    "    )  \n",
    "    \n",
    "    output_no_act = np.array(\n",
    "  [[[[  5.,   0.],\n",
    "   [  3.,   6.],\n",
    "   [ -3.,   1.]],\n",
    "\n",
    "  [[  6.,   5.],\n",
    "   [ -1.,   2.],\n",
    "   [-10.,  -4.]],\n",
    "\n",
    "  [[  3.,   5.],\n",
    "   [  1.,  -1.],\n",
    "   [  2.,   0.]]],\n",
    "\n",
    "\n",
    " [[[  5.,   0.],\n",
    "   [  3.,   6.],\n",
    "   [ -3.,   1.]],\n",
    "\n",
    "  [[  6.,   5.],\n",
    "   [ -1.,   2.],\n",
    "   [-10.,  -4.]],\n",
    "\n",
    "  [[  3.,   5.],\n",
    "   [  1.,  -1.],\n",
    "   [  2.,   0.]]],\n",
    "\n",
    "\n",
    " [[[  5.,   0.],\n",
    "   [  3.,   6.],\n",
    "   [ -3.,   1.]],\n",
    "\n",
    "  [[  6.,   5.],\n",
    "   [ -1.,   2.],\n",
    "   [-10.,  -4.]],\n",
    "\n",
    "  [[  3.,   5.],\n",
    "   [  1.,  -1.],\n",
    "   [  2.,   0.]]]]\n",
    "    )\n",
    "    \n",
    "    #Y = np.empty((2, 2, 3))\n",
    "    #Y[:, :, 0] = np.array([[1, 2], [3, 4]])\n",
    "    #Y[:, :, 1] = np.array([[5, 6], [7, 8]])\n",
    "    #Y[:, :, 2] = np.array([[9, 10], [11, 12]])\n",
    "    #X=Y\n",
    "                      \n",
    "    conv = CNNLayer(2, (3, 3, 3), 'no_act', stride=2)\n",
    "    conv.set_filters(np.stack((W0,W1), axis=0), np.array([b0, b1]))\n",
    "    output = (conv.forward_step(np.stack((X,X,X), axis=0)))\n",
    "    assert np.array_equal(output_no_act, output)\n",
    "    \n",
    "    conv = CNNLayer(2, (3, 3, 3), 'relu', stride=2)\n",
    "    conv.set_filters(np.stack((W0,W1), axis=0), np.array([b0, b1]))\n",
    "    output = (conv.forward_step(np.stack((X,X,X), axis=0)))\n",
    "    assert np.array_equal(output_relu, output)\n",
    "    \n",
    "    conv = CNNLayer(2, (3, 3, 3), 'relu', stride=2)\n",
    "    conv.set_filters(np.stack((W0,W1), axis=0), np.array([b0, b1]))\n",
    "    XX = X_no_pad\n",
    "    output = conv.forward_step(np.stack((XX,XX,XX), axis=0), 1, 1)\n",
    "    assert np.array_equal(output_relu, output)\n",
    "test_CNNLayer_cs231()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_CNNLayer_small():\n",
    "    \"\"\"\n",
    "    Small test example with depth=1 that tests forward and\n",
    "    backward steps assuming no activation.\n",
    "    \"\"\"\n",
    "    ######### WORD OF CAUTION #######\n",
    "    # In this and following tests the output of (del L/ del a_i)\n",
    "    # computed in the backward step corresponds to the shape of\n",
    "    # X after pads have been added to it, and not to that of the \n",
    "    # original X.\n",
    "    # So it contains some gradient values corresponding\n",
    "    # to the pads which are not really needed for gradient descent.\n",
    "    # To allow easy testing, please compute\n",
    "    # (del L/del a_i) corresponding to the padded X and not the \n",
    "    # original X.\n",
    "    \n",
    "    # X = a_i\n",
    "    X = np.array([[2,3],[4,1]])\n",
    "    # Two filters\n",
    "    F1 = np.array([[1, 0],[0,1]])\n",
    "    F2 = np.array([[1, 2],[0,1]])\n",
    "    # Expected a_j values\n",
    "    output_no_act_F1 = np.array(\n",
    "        [[[[ 2.],\n",
    "           [ 3.],\n",
    "           [ 0.]],\n",
    "          [[ 4.],\n",
    "           [ 3.],\n",
    "           [ 3.]],\n",
    "          [[ 0.],\n",
    "           [ 4.],\n",
    "           [ 1.]]]])\n",
    "    output_no_act_F2 = np.array(\n",
    "        [[[[ 2.],\n",
    "           [ 3.],\n",
    "           [ 0.]],\n",
    "          [[ 8.],\n",
    "           [ 9.],\n",
    "           [ 3.]],\n",
    "          [[ 8.],\n",
    "           [ 6.],\n",
    "           [ 1.]]]])\n",
    "    # Initialize cnn layer\n",
    "    conv = CNNLayer(1, (2,2,1), 'no_act', stride=1)\n",
    "\n",
    "    # Test forward step with filter F1 and bias 0\n",
    "    conv.set_filters(F1[np.newaxis,:,:,np.newaxis], np.array([0]))\n",
    "    X = X[np.newaxis,:,:,np.newaxis]\n",
    "    output = conv.forward_step(X, 1, 1)\n",
    "    assert np.array_equal(output, output_no_act_F1)\n",
    "\n",
    "    # Test both backward and forward steps with filter F2 and bias 0. \n",
    "    conv.set_filters(F2[np.newaxis,:,:,np.newaxis], np.array([0]))\n",
    "    output = conv.forward_step(X, 1, 1)\n",
    "    assert np.array_equal(output, output_no_act_F2)\n",
    "    # delta contains (del L)/(del a_j)\n",
    "    delta = np.array(\n",
    "    [[6, 0, 3],\n",
    "     [3, 0, 1],\n",
    "     [0, 0, 2]])\n",
    "    # Add axis to make it a 4-d tensor\n",
    "    DD = delta[np.newaxis,:,:,np.newaxis]\n",
    "    # DD_prev is expected (del L)/(del a_i)\n",
    "    DD_prev = np.array(\n",
    "        [[[  6.],\n",
    "           [ 12.],\n",
    "           [  3.],\n",
    "           [  6.]],\n",
    "          [[  3.],\n",
    "           [ 12.],\n",
    "           [  1.],\n",
    "           [  5.]],\n",
    "          [[  0.],\n",
    "           [  3.],\n",
    "           [  2.],\n",
    "           [  5.]],\n",
    "          [[  0.],\n",
    "           [  0.],\n",
    "           [  0.],\n",
    "           [  2.]]])    \n",
    "    new_delta = conv.backward_step(DD)\n",
    "    ##assert np.array_equal(new_delta, DD_prev[np.newaxis,:,:])\n",
    "test_CNNLayer_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_activation():\n",
    "\n",
    "# Test example with depth with no activation, testing all three function:\n",
    "# forward_step, backward_step, and filter gradient\n",
    "\n",
    "# The notation here is different, it corresponds to the pseudocode.\n",
    "\n",
    "# It is easier for humans to read tensors in the order\n",
    "# (example, depth, height, width).  So we write the\n",
    "# tensors in that order, but convert them to the order\n",
    "# (example, height, width, depth) expected by the code\n",
    "# before calling the functions.\n",
    "\n",
    "    L = np.array(\n",
    "        [ \n",
    "            [ # Example 0\n",
    "                  [ # Depth 0\n",
    "                        [1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [-1, -1, -1],\n",
    "                  [2,   2,  2],\n",
    "                  [0,   1,  3]\n",
    "                  ]\n",
    "            ],\n",
    "            [ # Example 1\n",
    "                  [ # Depth 0\n",
    "                        [19, 20, 21],\n",
    "                  [22, 23, 24],\n",
    "                  [25, 26, 27]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [28, 29, 30],\n",
    "                  [31, 32, 33],\n",
    "                  [34, 35, 36]\n",
    "                  ]\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "    F = np.array(\n",
    "        [ \n",
    "            [# Filter 0\n",
    "                 [ # Depth 0\n",
    "                       [1, -1],\n",
    "                 [2,  0]\n",
    "                 ],\n",
    "                 [ # Depth 1\n",
    "                       [ 0,  0],\n",
    "                 [-1, -1]\n",
    "                 ]\n",
    "            ],\n",
    "            [# Filter 1\n",
    "                 [ # Depth 0\n",
    "                       [0, -1],\n",
    "                 [3,  0]\n",
    "                 ],\n",
    "                 [ # Depth 1\n",
    "                       [ 1,  2],\n",
    "                 [-1, -1]\n",
    "                 ]\n",
    "            ]\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    delta_prime = np.array(\n",
    "        [ \n",
    "            [ # Example 0\n",
    "                  [ # Depth 0 (this corresponds to Filter 0, \n",
    "                        #          not L Depth 0)\n",
    "                  [-1, 1],\n",
    "                  [ 0, 2]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [0, -1],\n",
    "                  [1,  0]\n",
    "                  ]\n",
    "            ],\n",
    "            [ # Example 1\n",
    "                  [ # Depth 0\n",
    "                       [-2, 0],\n",
    "                  #[0,0],\n",
    "                  [ 0, 0]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [0,  1],\n",
    "                  [0,  0]\n",
    "                  ]\n",
    "            ]\n",
    "    \n",
    "        ])\n",
    "\n",
    "    F_delta_depth_first = np.array(\n",
    "        [\n",
    "            [ # Filter 0\n",
    "                  [ # Depth 0 (this corresponds to L Depth 0\n",
    "                        #          not delta_prime Depth 0)]\n",
    "                  [-27, -27],\n",
    "                  [-27, -27]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [-52, -54],\n",
    "                  [-60, -58]\n",
    "                  ]\n",
    "            ],\n",
    "            [ # Filter 1\n",
    "                  [ # Depth 0 \n",
    "                        [22, 23],\n",
    "                  [25, 26]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [32, 33],\n",
    "                  [30, 32]\n",
    "                  ]\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "    L_prime = np.array(\n",
    "        [[[[  4.,   3.],\n",
    "               [  6.,   5.]],\n",
    "\n",
    "              [[ 13.,  21.],\n",
    "                   [ 12.,  20.]]],\n",
    "\n",
    "\n",
    "        [[[-19.,  69.],\n",
    "              [-19.,  72.]],\n",
    "\n",
    "        [[-19.,  78.],\n",
    "             [-19.,  81.]]]]\n",
    "        )\n",
    "\n",
    "    L_delta = np.array(\n",
    "        [[[[-1.,  0.],\n",
    "               [ 2., -1.],\n",
    "               [ 0., -2.]],\n",
    "\n",
    "              [[-2.,  2.],\n",
    "                   [ 0.,  3.],\n",
    "              [-2.,  0.]],\n",
    "\n",
    "              [[ 3., -1.],\n",
    "                   [ 4., -3.],\n",
    "              [ 0., -2.]]],\n",
    "\n",
    "\n",
    "        [[[-2.,  0.],\n",
    "              [ 2.,  1.],\n",
    "              [-1.,  2.]],\n",
    "\n",
    "        [[-4.,  2.],\n",
    "             [ 3.,  1.],\n",
    "        [ 0., -1.]],\n",
    "\n",
    "        [[ 0.,  0.],\n",
    "             [ 0.,  0.],\n",
    "        [ 0.,  0.]]]]\n",
    "        )    \n",
    "\n",
    "    F_delta = np.array(\n",
    "        [[[[-27., -52.],\n",
    "               [-27., -54.]],\n",
    "\n",
    "              [[-27., -60.],\n",
    "                   [-27., -58.]]],\n",
    "\n",
    "\n",
    "        [[[ 22.,  32.],\n",
    "              [ 23.,  33.]],\n",
    "\n",
    "        [[ 25.,  30.],\n",
    "             [ 26.,  32.]]]]\n",
    "        )\n",
    "\n",
    "    # Convert L from (example, depth, height, width) to\n",
    "    # (example, height, width, depth)    \n",
    "    L = np.moveaxis(L,1,3)\n",
    "\n",
    "    # Convert F from (filter, depth, height, width) to\n",
    "    # (filter, height, width, depth)\n",
    "    F = np.moveaxis(F,1,3)\n",
    "\n",
    "    # Convert delta_prime from (example, depth, height, width) to\n",
    "    # (example, height, width, depth)\n",
    "    delta_prime = np.moveaxis(delta_prime, 1, 3)\n",
    "\n",
    "    conv = CNNLayer(2, (2, 2, 2), activation=\"no_act\")\n",
    "    conv.set_filters(F, biases = np.array([1, 0]))\n",
    "\n",
    "    L_prime_out = conv.forward_step(L)\n",
    "    L_delta_out = conv.backward_step(delta_prime)\n",
    "    F_delta_out = conv.filter_gradient(delta_prime)    \n",
    "\n",
    "    assert np.array_equal(L_prime, L_prime_out)\n",
    "    assert np.array_equal(L_delta, L_delta_out)\n",
    "    assert np.array_equal(F_delta, F_delta_out)\n",
    "\n",
    "test_no_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relu():\n",
    "    # Test example with depth with ReLU activation, testing all three function:\n",
    "    # forward_step, backward_step, and filter gradient\n",
    "    #\n",
    "    # The notation here is different, it corresponds to the pseudocode.\n",
    "    #\n",
    "    # It is easier for humans to read tensors in the order\n",
    "    # (example, depth, height, width).  So we write the\n",
    "    # tensors in that order, but convert them to the order\n",
    "    # (example, height, width, depth) expected by the code\n",
    "    # before calling the functions.\n",
    "    L = np.array(\n",
    "        [ \n",
    "            [ # Example 0\n",
    "                  [ # Depth 0\n",
    "                        [1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [-1, -1, -1],\n",
    "                  [2,   2,  2],\n",
    "                  [0,   1,  3]\n",
    "                  ]\n",
    "            ],\n",
    "            [ # Example 1\n",
    "                  [ # Depth 0\n",
    "                        [19, 20, 21],\n",
    "                  [22, 23, 24],\n",
    "                  [25, 26, 27]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [28, 29, 30],\n",
    "                  [31, 32, 33],\n",
    "                  [34, 35, 36]\n",
    "                  ]\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "    F = np.array(\n",
    "        [ \n",
    "            [# Filter 0\n",
    "                 [ # Depth 0\n",
    "                       [1, -1],\n",
    "                 [2,  0]\n",
    "                 ],\n",
    "                 [ # Depth 1\n",
    "                       [ 0,  0],\n",
    "                 [-1, -1]\n",
    "                 ]\n",
    "            ],\n",
    "            [# Filter 1\n",
    "                 [ # Depth 0\n",
    "                       [0, -1],\n",
    "                 [3,  0]\n",
    "                 ],\n",
    "                 [ # Depth 1\n",
    "                       [ 1,  2],\n",
    "                 [-1, -1]\n",
    "                 ]\n",
    "            ]\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    delta_prime = np.array(\n",
    "        [ \n",
    "            [ # Example 0\n",
    "                  [ # Depth 0 (this corresponds to Filter 0, \n",
    "                        #          not L Depth 0)\n",
    "                  [-1, 1],\n",
    "                  [ 0, 2]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [0, -1],\n",
    "                  [1,  0]\n",
    "                  ]\n",
    "            ],\n",
    "            [ # Example 1\n",
    "                  [ # Depth 0\n",
    "                        [-2, 0],\n",
    "                  [ 0, 0]\n",
    "                  ],\n",
    "                  [ # Depth 1\n",
    "                        [0,  1],\n",
    "                  [0,  0]\n",
    "                  ]\n",
    "            ]\n",
    "    \n",
    "        ])\n",
    "\n",
    "\n",
    "    L_prime = np.array(\n",
    "        [[[[  4.,   3.],\n",
    "               [  6.,   5.]],\n",
    "\n",
    "              [[ 13.,  21.],\n",
    "                   [ 12.,  20.]]],\n",
    "\n",
    "\n",
    "        [[[0.,  69.],\n",
    "              [0.,  72.]],\n",
    "\n",
    "        [[0.,  78.],\n",
    "             [0.,  81.]]]]\n",
    "        )\n",
    "\n",
    " \n",
    "    L_delta = np.array(\n",
    "        [[[[-1.,  0.],\n",
    "               [ 2., -1.],\n",
    "               [ 0., -2.]],\n",
    "\n",
    "              [[-2.,  2.],\n",
    "                   [ 0.,  3.],\n",
    "              [-2.,  0.]],\n",
    "\n",
    "              [[ 3., -1.],\n",
    "                   [ 4., -3.],\n",
    "              [ 0., -2.]]],\n",
    "\n",
    "\n",
    "        [[[ 0.,  0.],\n",
    "              [ 0.,  1.],\n",
    "              [-1.,  2.]],\n",
    "\n",
    "        [[ 0.,  0.],\n",
    "             [ 3., -1.],\n",
    "        [ 0., -1.]],\n",
    "\n",
    "        [[ 0.,  0.],\n",
    "             [ 0.,  0.],\n",
    "        [ 0.,  0.]]]])\n",
    "\n",
    "    F_delta = np.array(\n",
    "      [[[[11.,  4.],\n",
    "         [13.,  4.]],\n",
    "\n",
    "        [[17.,  2.],\n",
    "         [19.,  6.]]],\n",
    "\n",
    "\n",
    "       [[[22., 32.],\n",
    "         [23., 33.]],\n",
    "\n",
    "        [[25., 30.],\n",
    "         [26., 32.]]]])\n",
    "    \n",
    "    # Convert L from (example, depth, height, width) to\n",
    "    # (example, height, width, depth)    \n",
    "\n",
    "    L = np.moveaxis(L,1,3)\n",
    "\n",
    "    # Convert F from (filter, depth, height, width) to\n",
    "    # (filter, height, width, depth)\n",
    "    F = np.moveaxis(F,1,3)\n",
    "\n",
    "    # Convert delta_prime from (example, depth, height, width) to\n",
    "    # (example, height, width, depth)\n",
    "    delta_prime = np.moveaxis(delta_prime, 1, 3)\n",
    "\n",
    "    conv = CNNLayer(2, (2, 2, 2), activation='relu')\n",
    "    conv.set_filters(F, biases = np.array([1, 0]))\n",
    "\n",
    "    L_prime_out = conv.forward_step(L)\n",
    "    L_delta_out = conv.backward_step(delta_prime)\n",
    "    F_delta_out = conv.filter_gradient(delta_prime)    \n",
    "    \n",
    "    assert np.array_equal(L_prime, L_prime_out)\n",
    "    assert np.array_equal(L_delta, L_delta_out)\n",
    "    assert np.array_equal(F_delta, F_delta_out)\n",
    "    \n",
    "test_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# The following code uses the following concepts/functions from NumPy, and it\n",
    "# may help to brush up on them from the documentation.\n",
    "#\n",
    "# - Broadcasting\n",
    "# - np.pad\n",
    "# - np.reshape\n",
    "# - np.tensordot (optional)\n",
    "# - np.newaxis\n",
    "# - np.moveaxis\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "class relu:\n",
    "    \n",
    "    def function(self, X):\n",
    "        '''Return elementwise relu values'''\n",
    "        return(np.maximum(np.zeros(X.shape), X))\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        # An example of broadcasting\n",
    "        return((X >= 0).astype(int))\n",
    "        \n",
    "class no_act:\n",
    "    \"\"\"Implement a no activation function and derivative\"\"\"\n",
    "    \n",
    "    def function(self, X):\n",
    "        return(X)\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        return(np.ones(X.shape))\n",
    "\n",
    "# Set of allowed/implemented activation functions\n",
    "ACTIVATIONS = {'relu': relu,\n",
    "               'no_act': no_act}    \n",
    "    \n",
    "class CNNLayer:\n",
    "    \"\"\"\n",
    "    Implement a class that processes a single CNN layer.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    Additional two comments by Amitabh for Spring, 2020: \n",
    "    \n",
    "    (1) I emphasized slightly different \n",
    "    notation in my class presentations; this notebook uses the older version.  \n",
    "    For one, in class I have been using d(a_i) to denote (del L)/(del a_i).\n",
    "    Also in class I have emphasized that the nodes/neurons correspond to filters,\n",
    "    and the image/pixel values correspond to outputs when these filters are applied\n",
    "    at particular receptive field.  This notebook, however, calls each application\n",
    "    a \"neuron\".  So if an image corresponding to the jth layer consists of \n",
    "    500 x 500 x 3 pixels, the notebook imagines they are the outputs of 500 x 500 x 3 \n",
    "    \"neurons\". Which ignores the fact that there are actually only 3 distinct neurons, \n",
    "    (corresponding two 3 filters), each of which has been applied 500 x 500 times on\n",
    "    an input image.\n",
    "    \n",
    "    (2) This starter code uses the older idea that the activation function is part\n",
    "    of the layer.  I intend to change this in future.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    Let i be the index on the neurons in the ith layer, and j be the index on the \n",
    "    neurons in the next outer layer.  (Following Russell-Norvig notation.) Implement \n",
    "    the following:\n",
    "    \n",
    "    0. __init__: Initalize filters.\n",
    "    \n",
    "    1. forward step: Input a_i values.  Output a_j values.  Make copies of a_i values \n",
    "       and in_j values since needed in backward_step and filter_gradient.\n",
    "    \n",
    "    2. backward_step: Input (del L)/(del a_j) values.  Output (del L)/(del a_i).\n",
    "    \n",
    "    3. filter_gradient: Input (del L)/(del a_j) values. Output (del L)/(del w_{ij}) values.\n",
    "    \n",
    "    4. update: Given learning rate, update filter weights. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, filter_shape, activation='no_act', stride = 1):\n",
    "        \"\"\"\n",
    "        Initialize filters.\n",
    "        \n",
    "        filter_shape is (height of filter, width of filter, depth of filter). Depth \n",
    "        of filter should match depth of the forward_step input X.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_filters = n\n",
    "        self.stride = stride\n",
    "        self.filter_shape = filter_shape\n",
    "        try:\n",
    "            self.filter_height = filter_shape[0]\n",
    "            self.filter_width = filter_shape[1]\n",
    "            self.filter_depth = filter_shape[2]\n",
    "        except:\n",
    "            raise Exception(f'Unexpected filter shape {filter_shape}')\n",
    "        try:\n",
    "            # Create an object of the activation class\n",
    "            self.activation = ACTIVATIONS[activation]() \n",
    "        except:\n",
    "            raise Exception(f'Unknown activation: {activation}')\n",
    "        self.filters = self.filters_init()\n",
    "        self.biases = self.biases_init()\n",
    "        self.num_examples = None \n",
    "        # Set num_of_examples during forward step, and use to verify\n",
    "        # consistency during backward step.  Similarly the data height, \n",
    "        # width, and depth.\n",
    "        self.data_height = None\n",
    "        self.data_width = None\n",
    "        self.data_depth = None\n",
    "        self.data_with_pads = None\n",
    "        self.in_j = None  # the in_j values for next layer.\n",
    "        self.im2col = None\n",
    "        \n",
    "    def filters_init(self):\n",
    "        return np.random.random((self.num_filters, self.filter_height,\n",
    "                                 self.filter_width, self.filter_depth))\n",
    "    \n",
    "    def biases_init(self):\n",
    "        return np.random.random(self.num_filters)\n",
    "    \n",
    "    def set_filters(self, filters, biases):\n",
    "        \"\"\"Set filters to given weights.\n",
    "        \n",
    "           Useful in debugging.\"\"\"\n",
    "        if filters.shape != (self.num_filters, self.filter_height,\n",
    "                                 self.filter_width, self.filter_depth):\n",
    "            raise Exception(f'Mismatched filter shapes: stored '\n",
    "                            f'{self.num_filters} {self.filter_shape} vs '\n",
    "                            f'{filters.shape}.')\n",
    "        if biases.shape != (self.num_filters,):\n",
    "            raise Exception((f'Mismatched biases: stored '\n",
    "                             f'{self.num_filters} vs '\n",
    "                             f'{biases.shape}.'))\n",
    "        self.filters = filters.copy()\n",
    "        self.biases = biases.copy()\n",
    "        \n",
    "    def forward_step(self, X, pad_height=0, pad_width=0):\n",
    "        \"\"\"\n",
    "        Implement a forward step.\n",
    "        \n",
    "        X.shape is (number of examples, height of input, width of input, depth of input).\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Store shape values to verify consistency during backward step\n",
    "            self.num_examples = X.shape[0]\n",
    "            self.data_height = X.shape[1]\n",
    "            self.data_width = X.shape[2]\n",
    "            self.data_depth = X.shape[3]\n",
    "        except:\n",
    "            raise Exception(f'Unexpected data shape {X.shape}')\n",
    "        if self.data_depth != self.filter_depth:\n",
    "            raise Exception(f'Depth mismatch: filter depth {self.filter_depth}'\n",
    "                            f' data depth {self.data_depth}')\n",
    "        self.pad_height = pad_height\n",
    "        self.pad_width = pad_width\n",
    "        self.input_height = self.data_height + 2 * self.pad_height\n",
    "        self.input_width = self.data_width + 2 * self.pad_width\n",
    "        \n",
    "        # Add pad to X.  Only add pads to the 1, 2 (ht, width) axes of X, \n",
    "        # not to the 0, 4 (num examples, depth) axes.\n",
    "        # 'constant' implies 0 is added as pad.\n",
    "        X = np.pad(X, ((0,0),(pad_height, pad_height), \n",
    "                      (pad_width, pad_width), (0,0)), 'constant')\n",
    "        \n",
    "        # Save a copy for computing filter_gradient\n",
    "        self.a_i = X.copy()  #\n",
    "        \n",
    "        # Get height, width after padding\n",
    "        height = X.shape[1]\n",
    "        width = X.shape[2]\n",
    "\n",
    "        # Don't include pad in formula because height includes it.\n",
    "        output_height = ((height - self.filter_height)/self.stride + 1)\n",
    "        output_width = ((width - self.filter_width)/self.stride + 1)    \n",
    "        if (\n",
    "            output_height != int(output_height) or \n",
    "            output_width != int(output_width)\n",
    "        ):\n",
    "            raise Exception(f\"Filter doesn't fit: {output_height} x {output_width}\")\n",
    "        else:\n",
    "            output_height = int(output_height)\n",
    "            output_width = int(output_width)\n",
    "            \n",
    "        #####################################################################\n",
    "        # There are two ways to convolve the filters with X.\n",
    "        # 1. Using the im2col method described in Stanford 231 notes.\n",
    "        # 2. Using NumPy's tensordot method.\n",
    "        #\n",
    "        # (1) requires more code.  (2) requires understanding how tensordot\n",
    "        # works.  Most likely tensordot is more efficient.  To illustrate both,\n",
    "        # in the code below data_tensor is constructed using (1) and \n",
    "        # new_data_tensor is constructed using (2).  You may use either.\n",
    "            \n",
    "        # Stanford's im2col method    \n",
    "        # Construct filter tensor and add biases\n",
    "        filter_tensor = self.filters.reshape(self.num_filters, -1)\n",
    "        filter_tensor = np.hstack((self.biases.reshape((-1,1)), filter_tensor))\n",
    "        # Construct the data tensor\n",
    "        # The im2col_length does not include the bias terms\n",
    "        # Biases are later added to both data and filter tensors\n",
    "        im2col_length = self.filter_height * self.filter_width * self.filter_depth\n",
    "        num_outputs = output_height * output_width\n",
    "        data_tensor = np.empty((self.num_examples, num_outputs, im2col_length))\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                hs = h * self.stride\n",
    "                ws = w * self.stride\n",
    "                data_tensor[:,h*output_width + w, :] = X[:,hs:hs+self.filter_height,\n",
    "                                    ws:ws+self.filter_width,:].reshape(\n",
    "                                        (self.num_examples,-1))  \n",
    "        # add bias-coeffs to data tensor\n",
    "        self.im2col = data_tensor.copy()\n",
    "        data_tensor = np.concatenate((np.ones((self.num_examples, num_outputs, 1)),\n",
    "                                 data_tensor), axis=2)\n",
    "        \n",
    "        ## the im2col result, will use in backward\n",
    "        ## data_tensor: (num_examples,out_w*out_h,(C*kernel_h*kernel_w+1))\n",
    "        ## filter_tensor: (num_filters, C*kernel_h*kernel_w+1)\n",
    "        output_tensor = np.tensordot(data_tensor, filter_tensor, axes=([2],[1]))\n",
    "        output_tensor = output_tensor.reshape(\n",
    "            (self.num_examples,output_height,output_width,self.num_filters))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # NumPy's tensordot based method\n",
    "        new_output_tensor = np.empty((self.num_examples, output_height, \n",
    "                                      output_width, self.num_filters))\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                hs = h * self.stride\n",
    "                ws = w * self.stride\n",
    "                new_output_tensor[:,h,w,:] = np.tensordot(\n",
    "                                                X[:, # example\n",
    "                                                  hs:hs+self.filter_height, # height\n",
    "                                                  ws:ws+self.filter_width,  # width\n",
    "                                                  : # depth\n",
    "                                                ], \n",
    "                                                self.filters[:, # filter \n",
    "                                                             :, # height\n",
    "                                                             :, # width\n",
    "                                                             :  # depth\n",
    "                                                ], \n",
    "                                                axes = ((1,2,3),(1,2,3))\n",
    "                                              )\n",
    "                # Add bias term\n",
    "                new_output_tensor[:,h,w,:] = (new_output_tensor[:,h,w,:] + \n",
    "                                              self.biases)\n",
    "        # Check both methods give the same answer\n",
    "        assert np.array_equal(output_tensor, new_output_tensor)\n",
    "                \n",
    "        \n",
    "        self.in_j = output_tensor.copy() # Used in backward_step.\n",
    "        output_tensor = self.activation.function(output_tensor) # a_j values\n",
    "        return(output_tensor)\n",
    "      \n",
    "    def backward_step(self, D):\n",
    "        \"\"\"\n",
    "        Implement the backward step and return (del L)/(del a_i). \n",
    "        \n",
    "        Given D=(del L)/(del a_j) values return (del L)/(del a_i) values.  \n",
    "        D (delta) is of shape (number of examples, height of output (i.e., the \n",
    "        a_j values), width of output, depth of output).\n",
    "        \n",
    "        Note that in our tests we will assume that (del L)/(del a_i) has\n",
    "        shape corresponding to the padded X used in forward_step, and not the\n",
    "        unpadded X.  Strictly speaking, only the unpadded X gradient is needed\n",
    "        for gradient descent, but here we ask you to compute the gradient for the \n",
    "        padded X.  All our tests below, and in our grading will make this assumption.\n",
    "        \n",
    "        YOU MAY ASSUME STRIDE IS SET TO 1.        \n",
    "        \"\"\"\n",
    "         ##  A = self.in_j : (self.num_examples,output_height,output_width,self.num_filters)\n",
    "        try:\n",
    "            num_examples = D.shape[0]\n",
    "            delta_height = D.shape[1]\n",
    "            delta_width = D.shape[2]\n",
    "            delta_depth = D.shape[3]\n",
    "        except:\n",
    "            raise Exception(f'Unexpected delta shape {D.shape}')\n",
    "        if num_examples != self.num_examples:\n",
    "            raise Exception(f'Number of examples changed from forward step: '\n",
    "                             f'{self.num_examples} vs {num_examples}')\n",
    "        if delta_depth != self.num_filters:\n",
    "            raise Exception(f'Depth mismatch: number of filters {self.num_filters}' \n",
    "                            f' delta depth {delta_depth}')\n",
    "        # Make a copy so that we can change it\n",
    "        prev_delta = D.copy()\n",
    "        if prev_delta.ndim != 4:\n",
    "            raise Exception(f'Unexpected number of dimensions {D.ndim}')\n",
    "        new_delta = None\n",
    "        \n",
    "        ####################################################################\n",
    "        # WRITE YOUR CODE HERE\n",
    "        #   D (delta) is of shape (number of examples, height of output (i.e., the a_j values), width of output, depth of output = #of filters)\n",
    "        ## new_delta if of shape  :(number of examples, height of input, width of input, depth of input).\n",
    "        ## self.filters : (num_filters, filter_height, filter_width, filter_depth=input_depth)\n",
    "        prev_delta = self.activation.derivative(self.in_j)*prev_delta\n",
    "        new_delta =  np.zeros((self.a_i.shape))\n",
    "        for i in range(D.shape[0]):\n",
    "            for h in range(D.shape[1]):\n",
    "                for w in range(D.shape[2]):\n",
    "                    for c in range(D.shape[3]):\n",
    "                        new_delta[i,h:h+self.filter_height,w:w+self.filter_width,:] += (self.filters[c,:,:,:]*prev_delta[i,h,w,c])             \n",
    "        return(new_delta)\n",
    "    \n",
    "    def filter_gradient(self, D):\n",
    "        \"\"\"\n",
    "        Return the filter_gradient.\n",
    "        \n",
    "        D = (del L)/(del a_j) has shape (num_examples, height, width, depth=num_filters)\n",
    "        The filter_gradient (del L)/(del w_{ij}) has shape (num_filters, filter_height, \n",
    "        filter_width, filter_depth=input_depth)\n",
    "        \n",
    "        YOU MAY ASSUME STRIDE IS SET TO 1.\n",
    "        \n",
    "        \"\"\"\n",
    "         \n",
    "        if DEBUG and D.ndim != 4:\n",
    "            raise Exception(f'D has {D.ndim} dimensions instead of 4.')\n",
    "        # D depth should match number of filters\n",
    "        D_depth = D.shape[3]\n",
    "        if DEBUG:\n",
    "            if D_depth != self.num_filters:\n",
    "                raise Exception(f'D depth {D_depth} != num_filters'\n",
    "                                f' {self.num_filters}')\n",
    "            if D.shape[0] != self.num_examples:\n",
    "                raise Exception(f'D num_examples {D.shape[0]} !='\n",
    "                                f'num_examples {self.num_examples}')\n",
    "        f_gradient = None\n",
    "        ####################################################################\n",
    "        # WRITE YOUR CODE HERE\n",
    "        ## self.ai:(number of examples, height of input, width of input, depth of input).\n",
    "        ## D: (num_examples, height, width,  D_depth)\n",
    "        ## self.im2col: (num_examples,out_w*out_h,(C*kernel_h*kernel_w))\n",
    "        ## output f_gradient: (num_filters, filter_height, filter_width, filter_depth=input_depth)\n",
    "        prev_delta = D.copy()\n",
    "        prev_delta = self.activation.derivative(self.in_j)*prev_delta\n",
    "        f_gradient = np.zeros((D_depth,self.filter_height,self.filter_width,self.filter_depth))\n",
    "        bias_temp = np.zeros((D_depth))\n",
    "        for i in range(D.shape[0]):\n",
    "            for c in range(D_depth):\n",
    "                for h in range(D.shape[1]):\n",
    "                    for w in range(D.shape[2]):\n",
    "                        temp = self.im2col[i,(h*D.shape[0] + w),:].reshape(self.filter_depth,self.filter_height,self.filter_width)\n",
    "                        f_gradient[c,:,:,:] += temp*prev_delta[i,h,w,c]\n",
    "                        bias_temp[c] += D[i,h,w,c]\n",
    "        ## f_gradient = np.concatenate((f_gradient,bias_temp), axis=0)\n",
    "        return f_gradient\n",
    "                    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
